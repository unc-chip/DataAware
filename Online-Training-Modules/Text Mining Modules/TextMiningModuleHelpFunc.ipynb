{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(filename=\"./Static/Helpfunction.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to c:\\users\\dapeng\\appdata\\l\n",
      "[nltk_data]     ocal\\programs\\python\\python37\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-da9523b60fd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcontractions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCONTRACTION_MAP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "import re # import \"re\" function\n",
    "import nltk # import nltk library\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "\n",
    "import spacy                    #import spacy module\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from contractions import CONTRACTION_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case: \n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "  \n",
    "    return ' '.join(filtered_tokens)  \n",
    "\n",
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer() \n",
    "    \n",
    "    return ' '.join([ps.stem(word) for word in text.split()]) \n",
    "   \n",
    "def lemmatize_text(text):\n",
    "    s = \" \"\n",
    "    t_l = []\n",
    "    t_w = nltk.word_tokenize(text) \n",
    "    for w in t_w:\n",
    "        l_w = wordnet_lemmatizer.lemmatize(w, pos=\"v\")\n",
    "        t_l.append(l_w)\n",
    "        \n",
    "    return s.join(t_l)  \n",
    "\n",
    "def expand_contractions(text):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = CONTRACTION_MAP.get(match)\\\n",
    "                                if CONTRACTION_MAP.get(match)\\\n",
    "                                else CONTRACTION_MAP.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "\n",
    "    return re.sub(\"'\", \"\", expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_ngram(document, N = 1, allgram = False): \n",
    "    \n",
    "    agg_words = ' '.join([text for text in document]) \n",
    "    tok_list = agg_words.split() \n",
    "    ngram_tok_list = [' '.join(toks) for toks in ngrams(tok_list, N)]\n",
    "    if allgram and N>1:\n",
    "        ngram = [ngrams(tok_list, i) for i in range(1,N)]\n",
    "        ngram_tok_list.extend(' '.join(toks) for ng in ngram for toks in ng)\n",
    "    fdist = nltk.FreqDist(ngram_tok_list) \n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()), \n",
    "                             'count':np.array(list(fdist.values())), \n",
    "                             'frequency': np.array(list(fdist.values()))/sum(fdist.values())}) \n",
    "    return words_df\n",
    "\n",
    "    \n",
    "def viz_ngram_freq(df, figsize = (8,8)):\n",
    "    plt.figure(figsize = figsize) \n",
    "    ax = sns.barplot(data=df, x= \"count\", y = \"word\") \n",
    "    ax.set(ylabel = 'Word') \n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_matrix(document, tfidf = True):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(document) if tfidf else CountVectorizer(document)\n",
    "    bow_matrix = vectorizer.fit_transform(document)\n",
    "    df = pd.DataFrame(bow_matrix.toarray(), columns = vectorizer.get_feature_names())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_text_similarity(doc_a, doc_b, method = \"cosine\"):\n",
    "    \n",
    "    if isinstance(doc_a, str):\n",
    "        doc_a = [doc_a]\n",
    "    if isinstance(doc_b, str):\n",
    "        doc_b = [doc_b]\n",
    "    doc = doc_a + doc_b\n",
    "    bow_matrix = create_bow_matrix(doc)\n",
    "    sim_matrix = cosine_similarity(bow_matrix) if method == \"cosine\" else euclidean_distances(bow_matrix)\n",
    "    \n",
    "    return pd.DataFrame(sim_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkdoc(f):\n",
    "    def wrapper(*args):\n",
    "        if not args[0].nlpdoc:\n",
    "            raise ValueError((\"\"\"\n",
    "            The document is empty. \n",
    "            You should use start_extract() to initiate the extracter before you use this function\n",
    "            \"\"\"))\n",
    "        return f(*args)\n",
    "    return wrapper\n",
    "    \n",
    "class ExtactInfo:\n",
    "    ''' ExtractInfo class'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlpdoc = []\n",
    "\n",
    "    def start_extract(self, document):\n",
    "        \n",
    "        if isinstance(document, str):\n",
    "            document = [document]        \n",
    "        \n",
    "        self.nlpdoc = list(nlp.pipe(document))\n",
    "    \n",
    "    @checkdoc\n",
    "    def get_postag(self, postagtype = \"univ\"):\n",
    "\n",
    "        pos_tag = []\n",
    "        if postagtype == \"univ\":\n",
    "            for doc in self.nlpdoc:\n",
    "                pos_tag.append(dict(Counter([tok.pos_ for tok in doc])))\n",
    "        else:\n",
    "            for doc in self.nlpdoc:\n",
    "                pos_tag.append(dict(Counter([tok.tag_ for tok in doc])))            \n",
    "        \n",
    "        return pd.DataFrame(pos_tag, dtype='Int64').fillna(0)\n",
    "    \n",
    "    @checkdoc\n",
    "    def get_noun_phrase(self):\n",
    "        return [list(doc.noun_chunks) for doc in self.nlpdoc] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "HYPERPARAM = {\n",
    "   'logistic':{\n",
    "    'logistic__penalty': ['l2'],\n",
    "    'logistic__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'logistic__class_weight': ['balanced'],\n",
    "    'logistic__n_jobs': [-1]\n",
    "    },\n",
    "    \"kn\":{\n",
    "      'kn__n_neighbors':[5,11,19],\n",
    "      'kn__weights':['uniform', 'distance'],\n",
    "      'kn__algorithm':['ball_tree'], #'auto','kd_tree','brute'\n",
    "      'kn__n_jobs':[-1]\n",
    "    }\n",
    "} \n",
    "SCORE_FUNC = {\n",
    "    \"chi2\": chi2,\n",
    "    \"Fvalue\": f_classif\n",
    "}\n",
    "MODEL_METHOD = {\n",
    "    'kn':KNeighborsClassifier(),\n",
    "    'logistic':LogisticRegression()\n",
    "}\n",
    "def checkmodel(f):\n",
    "    def wrapper(*args):\n",
    "        if not args[0].select_model:\n",
    "            raise ValueError((\"\"\"\n",
    "            The model has not been trained. You need to fit_model first with training dataset before prediction\n",
    "            \"\"\"))\n",
    "        return f(*args)\n",
    "    return wrapper\n",
    "\n",
    "class classifier():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.y_pred = None\n",
    "        self.method = None\n",
    "        self.select_model = None\n",
    "        self.feature_set = []\n",
    "        self.fine_tune_res = None\n",
    "        \n",
    "    def fit_model(self, x_train, y_train, \n",
    "                  method = \"kn\", \n",
    "                  score_func = 'chi2', \n",
    "                  see_tuning = False,\n",
    "                  feature_list = []):\n",
    "        ''' Fit model with training data, identified method and score function; \n",
    "            This include select feature and parameter tuning\n",
    "        '''\n",
    "        if method not in MODEL_METHOD.keys():\n",
    "            raise ValueError(\"\"\"\n",
    "            Choose method from 'kn' or 'logistic'\n",
    "            \"\"\")\n",
    "        if score_func not in SCORE_FUNC.keys():\n",
    "            raise ValueError(\"\"\"\n",
    "            Choose score function from 'chi2' or 'Fvalue'\n",
    "            \"\"\")\n",
    "\n",
    "        self.x_train = x_train.copy()\n",
    "        self.y_train = y_train.copy()\n",
    "        self.method = method\n",
    "\n",
    "        (n_samples, n_features) = x_train.shape\n",
    "        \n",
    "        feat_selector = SelectKBest(SCORE_FUNC.get(score_func), min(n_samples-1, n_features))    # Feature length is better off less than sample size\n",
    "        \n",
    "        clf = MODEL_METHOD[method]\n",
    "        \n",
    "        pipe = Pipeline(steps=[('selector', feat_selector), \n",
    "                                 (method, clf)])   # clf should have the same name as HYPERPARAM keys\n",
    "        \n",
    "        if not HYPERPARAM:\n",
    "            raise ValueError(\"Parameters are required to finetune\")\n",
    "        grid_search = GridSearchCV(pipe, \n",
    "                                   param_grid=HYPERPARAM[method], \n",
    "                                   verbose = 1)\n",
    "        grid_search.fit(x_train, y_train)\n",
    "        self.select_model = grid_search.best_estimator_\n",
    "        self.fine_tune_res = pd.DataFrame(grid_search.cv_results_).sort_values(by='rank_test_score')\n",
    "        if feature_list:\n",
    "            self.feature_set = features_list[feat_selector.get_support()]\n",
    "        if see_tuning:\n",
    "            import pprint\n",
    "            print(self.fine_tune_res)\n",
    "            print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "            pprint.pprint(\"Best parameters set:\", grid_search.best_estimator_.get_params())\n",
    "                   \n",
    "    \n",
    "    @checkmodel\n",
    "    def predict(self, x_test, y_test):\n",
    "        '''\n",
    "        predict and evaluate score\n",
    "        \n",
    "        '''\n",
    "        self.y_pred = self.select_model.predict(x_test)\n",
    "        score = accuracy_score(y_test, self.y_pred)\n",
    "        \n",
    "        print(\"Prediction accuracy is {}\".format(score))\n",
    "        \n",
    "        return self.y_pred\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
